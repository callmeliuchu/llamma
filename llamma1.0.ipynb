{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05d62c4-5ecd-4b6a-963f-90017d33e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83975ca3-bc9f-47b3-8749-828327765255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663d9484-126d-4823-bad3-8025146e0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca038292-879e-493c-b81f-985798bac8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859cbe9e-b29a-4705-816a-e392ad030300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b4e81-ec1d-42d1-a369-84445da82429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f70277-2faa-43f5-aaad-c568e717f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5a73d7-bee6-44ab-ae5a-5a8d9e1160cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_llama import LlamaPreTrainedModel, LlamaConfig\n",
    "from rope import apply_rotary_emb\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0d5f775-274e-4ea7-ac78-4642f13a33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,eps):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    def forward(self,x):\n",
    "        return  self.weights * x / ((x ** 2).mean(dim=-1,keepdim=True) ** 0.5 + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0b6865d-63c8-43e5-a625-fa89c2589b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f31c4e5-92de-4afd-b41d-55146452a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = RMSNorm(4,0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be1bab3d-ecd6-49eb-b053-1afaa06f7008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5115],\n",
      "        [0.6114],\n",
      "        [0.5034],\n",
      "        [0.5390],\n",
      "        [0.7196]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rms(x) ** 2).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34240038-ecb7-4f29-88ff-f4f1dc88d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(x,dim,n):\n",
    "    B,T,head,head_dim = x.shape\n",
    "    return x[:,:,:,None,:].expand(B,T,head,n,head_dim).reshape(B,T,head*n,head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e11d481e-a137-4c80-9507-32dbc036ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,config:LlamaConfig):\n",
    "        ### group attention\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_kv_heads\n",
    "        assert self.n_heads % self.n_kv_heads == 0\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.wq = nn.Linear(self.dim,self.n_heads * self.head_dim,bias=False)\n",
    "        self.wk = nn.Linear(self.dim,self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wv = nn.Linear(self.dim,self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wo = nn.Linear(self.n_heads * self.head_dim,self.dim,bias=False)\n",
    "        \n",
    "    def compute(self,q,k,v):\n",
    "        B,head,T,head_dim = q.shape\n",
    "        scores = q @ k.transpose(2,3) # B,head,T,T\n",
    "        mask = torch.tril(torch.ones(T,T))\n",
    "        scores = scores.masked_fill(mask == 0.,'-inf')\n",
    "        scores = F.softmax(scores,dim=-1)\n",
    "        return scores @ v\n",
    "    \n",
    "        \n",
    "    def forward(self,x):\n",
    "        ### x (B,T,dim)\n",
    "        B,T,dim = x.shape\n",
    "        q = self.wq(x).view(B,T,self.n_heads,self.head_dim)\n",
    "        k = self.wk(x).view(B,T,self.n_kv_heads,self.head_dim)\n",
    "        v = self.wv(x).view(B,T,self.n_kv_heads,self.head_dim)\n",
    "        k = repeat(k)\n",
    "        v = repeat(v)\n",
    "        # B,T,head,head_dim\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # B,head,T,head_dim\n",
    "        x = self.compute(q,k,v)\n",
    "        # B,T dim\n",
    "        return self.wo(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d130551c-b5e4-4109-a5d0-3ff395f6310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self,dim:int, hidden_dim:int, multiple_of:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim,hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim,dim)\n",
    "        self.w3 = nn.Linear(dim,hidden_dim)\n",
    "    def SiluGlu(self,x):\n",
    "        return F.silu(self.w1(x)) * self.w3(x)\n",
    "    def forward(self,x):\n",
    "        return self.w2(self.SiluGlu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67b19382-2152-4338-846e-87e1df46e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaLayer(nn.Module):\n",
    "    def __init__(self,layer_id:int,config:LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.attn_norm = RMSNorm(config.dim,eps=0.00001)\n",
    "        self.feed_norm = RMSNorm(config.dim,eps=0.00001)\n",
    "        self.attn = Attention(config)\n",
    "        self.feed = FeedForward(config.dim,config.hidden_dim,config.multiple_of,config.dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.feed(self.feed_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a48dc1c2-58fd-477d-abe1-a0c1c0992f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama(LlamaPreTrainedModel):\n",
    "    def __init__(self,config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(config.n_layers):\n",
    "            self.layers.append(LlamaLayer(i,config))\n",
    "        self.norm = RMSNorm(config.dim,eps=0.000001)\n",
    "        self.wo = nn.Linear(config.dim,config.vocab_size)\n",
    "        self.word_embedding = nn.Embedding(config.vocab_size,config.dim)\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.wo(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self,tokens,max_len):\n",
    "        # B,T\n",
    "        for _ in range(max_len):\n",
    "            x = self.word_embedding(tokens) # B,T,dim\n",
    "            logits = self.forward(x) # B,T,vocabsize\n",
    "            logits = logits[:,-1,:] # B, vocabsize\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx = torch.multinomial(probs,num_samples=1) # B,1\n",
    "            tokens = torch.cat((torch,idx),dim=-1)\n",
    "        return  tokens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d23bcc4-51b1-42b7-b49c-857ccb516c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(checkpoint):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92405cbf-9aeb-4994-8e93-c0bfbfd59c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minitorch Environment",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
